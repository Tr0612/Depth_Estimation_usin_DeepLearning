{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzI13KFOj4Pj"
      },
      "source": [
        "# Coded Deep Depth Estimation [70pts]\n",
        "\n",
        "This is the part 2 of the project 2. Part 1 is this project is present [here](https://colab.research.google.com/drive/1M9t9gxG-1Yeb7DXVzPOBNC77q97Pq7xj?usp=sharing).\n",
        "\n",
        "The aim of this section is two folds:\n",
        "1. Given All in focus (AiF) or pinhole model RGB images and Ground Truth Depth maps, train your monocular depth estimation method using NYUv2 and UMDCodedVO-LivingRoom dataset only and test them on out of domain ICL-NUIM and UMDCodedVO-DiningRoom datasets. ((Link to test set)[https://drive.google.com/drive/folders/12U8BH-AWUA4DgbOValO-_hNI_Z9RgMXr?usp=sharing]) [25pts]\n",
        "\n",
        "2. Given an Coded RGB images and Ground Truth Depth maps, train your monocular depth estimation method using NYUv2 and UMDCodedVO-LivingRoom dataset only and test them on out of domain ICL-NUIM and UMDCodedVO-DiningRoom datasets. (Use the code for Part 1 to generate coded images for ICL-NUIM and UMDCodedVO-DiningRoom datasets) [25pts]\n",
        "\n",
        "Evaluate and compare results on testing dataset for both the above mentioned monocular depth methods. Use Rel-Abs and RMSE metric for quantitative evaluation. Use 'viridis' or 'plasma' color scheme for qualitative evaluation [link](https://matplotlib.org/stable/users/explain/colors/colormaps.html). Note: Both qualitative and quantitative evaluation must be with metric units i.e. from 0m to 6m. [20pts]\n",
        "\n",
        "\n",
        "## Note: Use of A100 is highly recommended"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tCBrysr87bB"
      },
      "source": [
        "## A. Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjn7UM8jMen8",
        "outputId": "fa16d1de-71e1-471c-f300-b1620542c273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os  # For setting environment variables and interacting with the operating system\n",
        "import torch  # PyTorch for deep learning computations\n",
        "import torch.nn as nn  # Neural network modules from PyTorch\n",
        "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"] = \"1\"  # Enable OpenEXR format support in OpenCV\n",
        "import cv2  # OpenCV for computer vision tasks\n",
        "import numpy as np  # For numerical operations\n",
        "import torchvision.transforms as transforms  # For data augmentation and transformation\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Dataset  # PyTorch utilities for data handling\n",
        "from tqdm import tqdm  # Progress bar library to monitor loops\n",
        "import time  # To track time during execution\n",
        "\n",
        "# Environment Setup\n",
        "\n",
        "# Select device for computation, CUDA if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)  # Print the selected device (either 'cuda' or 'cpu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GsuwJH_9zyT"
      },
      "source": [
        "## B. Data Handling\n",
        "(Nothing to modify here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8tP6njC9eo3"
      },
      "outputs": [],
      "source": [
        "# Data Loader class for handling image and depth pairs\n",
        "class ImageDepthDataset(Dataset):\n",
        "    def __init__(self, path: str, codedDir: str = \"Coded\", cache: bool = True, is_blender: bool = False,\n",
        "                 image_size=(480, 640), scale_factor: float = 5000, limit=None):\n",
        "        \"\"\"\n",
        "        Initializes the dataset by loading image and depth file pairs.\n",
        "\n",
        "        Parameters:\n",
        "        - path: Directory where the dataset is stored.\n",
        "        - codedDir: Directory within the path that contains the coded images (default: \"Coded\").\n",
        "        - cache: Whether to load and store the data in memory (True) or process on-the-fly (False).\n",
        "        - is_blender: Flag to indicate whether the depth data is from Blender (EXR format) or not (PNG format).\n",
        "        - image_size: Desired size of the images after cropping (default: (480, 640)).\n",
        "        - scale_factor: Scale factor to adjust depth values (default: 5000).\n",
        "        - limit: Maximum number of image-depth pairs to load. If None, loads all available files.\n",
        "        \"\"\"\n",
        "        self.path = path  # Path to the dataset\n",
        "        self.is_blender = is_blender  # Whether the depth images are from Blender\n",
        "        self.transform = transforms.Compose([transforms.CenterCrop(image_size)])  # Image transformation (cropping)\n",
        "        self.data = []  # Container to store processed or unprocessed file paths/data\n",
        "        self.scale_factor = scale_factor  # Scaling factor to adjust depth values\n",
        "        self.limit = limit if limit else -1  # Limit on the number of files to load, if provided\n",
        "        dir_path = os.path.join(path, codedDir)  # Path to the directory containing coded images\n",
        "        print(codedDir, is_blender, scale_factor)  # Print dataset configuration for debugging\n",
        "\n",
        "        # Get list of coded image files (sorted) from the directory, apply limit if set\n",
        "        files = sorted([p for p in os.listdir(dir_path) if p.endswith(\".png\")])[:self.limit]\n",
        "\n",
        "        # Iterate through the files and append their coded and depth file paths to self.data\n",
        "        for file in files:\n",
        "            coded_file = os.path.join(path, codedDir, file)  # Full path to the coded image\n",
        "            # Path to corresponding depth file (EXR for Blender, otherwise PNG)\n",
        "            depth_file = os.path.join(path, \"depth\", file.replace(\".png\", \".exr\") if is_blender else file)\n",
        "\n",
        "            # If caching is enabled, process and store the data in memory\n",
        "            if cache:\n",
        "                self.data.append(self.process(coded_file, depth_file))\n",
        "            # Otherwise, store the file paths for on-the-fly processing\n",
        "            else:\n",
        "                self.data.append((coded_file, depth_file))\n",
        "\n",
        "        self.cache = cache  # Whether the dataset is cached in memory or not\n",
        "        self.len = len(self.data)  # Number of files loaded\n",
        "        print(self.len)  # Print the number of files loaded for debugging\n",
        "\n",
        "    def process(self, coded_file: str, depth_file: str):\n",
        "        \"\"\"\n",
        "        Processes an image-depth pair by loading, transforming, and scaling them.\n",
        "\n",
        "        Parameters:\n",
        "        - coded_file: Path to the coded image file.\n",
        "        - depth_file: Path to the corresponding depth image file.\n",
        "\n",
        "        Returns:\n",
        "        A dictionary with the transformed and scaled image and depth.\n",
        "        \"\"\"\n",
        "        # Load coded image as a tensor (permute to change dimensions from HxWxC to CxHxW)\n",
        "        coded = torch.from_numpy(cv2.imread(coded_file)).permute(2, 0, 1)\n",
        "        # print(f\"Coded Image Shape (before transform): {coded.shape}\")\n",
        "        # Process depth based on the is_blender flag (EXR or PNG format)\n",
        "        if self.is_blender:\n",
        "            # Load depth from EXR format (for Blender data)\n",
        "            raw_depth = cv2.imread(depth_file, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH)\n",
        "            metric_depth = torch.from_numpy(raw_depth[:, :, 0])  # Extract depth channel\n",
        "        else:\n",
        "            # Load depth and apply scale factor (for non-Blender data)\n",
        "            metric_depth = torch.from_numpy(cv2.imread(depth_file, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH) / self.scale_factor)\n",
        "        # print(f\"Depth Image Shape (before transform): {metric_depth.shape}\")\n",
        "        # Return a dictionary with processed image and depth data\n",
        "        return {\n",
        "            \"image_id\": coded_file.split('/')[-1][:-4],  # Extract image ID from file name\n",
        "            \"Coded\": self.transform(coded.to(torch.float32)) / 255.0,  # Normalize the coded image\n",
        "            \"Depth\": self.transform(metric_depth.to(torch.float32))  # Transform the depth image\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a single sample from the dataset by index.\n",
        "\n",
        "        Parameters:\n",
        "        - idx: Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "        The processed image-depth pair if cached, otherwise processes on the fly.\n",
        "        \"\"\"\n",
        "        # If cached, return the pre-processed data\n",
        "        if self.cache:\n",
        "            return self.data[idx]\n",
        "        # Otherwise, process the data on-the-fly and return\n",
        "        else:\n",
        "            return self.process(*self.data[idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-pnsuHuEp61"
      },
      "source": [
        "## C. Network architecture goes here:\n",
        "\n",
        "Follow network details from [here](https://arxiv.org/pdf/2407.18240)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaqTPzKtOdUj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF  # Import transforms for cropping\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class U_Net(nn.Module):\n",
        "    def __init__(self, img_ch=3, output_ch=1):\n",
        "        super(U_Net, self).__init__()\n",
        "        self.encoder1 = ConvBlock(img_ch, 64)\n",
        "        self.encoder2 = ConvBlock(64, 128)\n",
        "        self.encoder3 = ConvBlock(128, 256)\n",
        "        self.encoder4 = ConvBlock(256, 512)\n",
        "        self.encoder5 = ConvBlock(512, 1024)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.upconv5 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.decoder5 = ConvBlock(1024, 512)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.decoder4 = ConvBlock(512, 256)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.decoder3 = ConvBlock(256, 128)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.decoder2 = ConvBlock(128, 64)\n",
        "\n",
        "        # self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
        "        self.decoder1 = ConvBlock(128, 64)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, output_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        # print(f\"Input shape: {x.shape}\")\n",
        "        enc1 = self.encoder1(x)\n",
        "        # print(f\"After encoder1: {enc1.shape}\")# (1, 64, 480, 640)\n",
        "        enc2 = self.encoder2(self.pool(enc1))\n",
        "        # print(f\"After encoder2: {enc2.shape}\")# (1, 128, 240, 320)\n",
        "        enc3 = self.encoder3(self.pool(enc2))\n",
        "        # print(f\"After encoder3: {enc3.shape}\")# (1, 256, 120, 160)\n",
        "        enc4 = self.encoder4(self.pool(enc3))\n",
        "        # print(f\"After encoder4: {enc4.shape}\")# (1, 512, 60, 80)\n",
        "        enc5 = self.encoder5(self.pool(enc4))\n",
        "        # print(f\"After encoder5: {enc5.shape}\")# (1, 1024, 30, 40)\n",
        "\n",
        "        # Decoder\n",
        "        dec5 = self.upconv5(enc5)\n",
        "        # print(f\"After up5: {dec5.shape}\")# (1, 512, 60, 80)\n",
        "        dec5 = torch.cat((enc4, dec5), dim=1)\n",
        "        # print(f\"After skip: {dec5.shape}\")# Skip connection (1, 1024, 60, 80)\n",
        "        dec5 = self.decoder5(dec5)\n",
        "        # print(f\"After dec5: {dec5.shape}\")# (1, 512, 60, 80)\n",
        "\n",
        "        dec4 = self.upconv4(dec5)\n",
        "        # print(f\"After up4: {dec4.shape}\")# (1, 256, 120, 160)\n",
        "        dec4 = torch.cat((enc3, dec4), dim=1)\n",
        "        # print(f\"After skip: {dec4.shape}\")# Skip connection (1, 512, 120, 160)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        # print(f\"After dec4: {dec4.shape}\")# (1, 256, 120, 160)\n",
        "\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        # print(f\"After up: {dec3.shape}\")# (1, 128, 240, 320)\n",
        "        dec3 = torch.cat((enc2, dec3), dim=1)\n",
        "        # print(f\"After skip: {dec3.shape}\")# Skip connection (1, 256, 240, 320)\n",
        "        dec3 = self.decoder3(dec3)  # (1, 128, 240, 320)\n",
        "        # print(f\"After dec3: {dec3.shape}\")\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        # print(f\"After up2: {dec2.shape}\")# (1, 64, 480, 640)\n",
        "        dec2 = torch.cat((enc1, dec2), dim=1)\n",
        "        # print(f\"After skip: {dec2.shape}\")# Skip connection (1, 128, 480, 640)\n",
        "        # dec2 = self.decoder2(dec2)  # (1, 64, 480, 640)\n",
        "        # print(f\"After dec2: {dec2.shape}\")\n",
        "\n",
        "        # dec1 = self.upconv1(dec2)  # (1, 64, 960, 1280)\n",
        "        # print(f\"After up: {dec1.shape}\")\n",
        "        # Apply center cropping to match the size of x (original image)\n",
        "        # dec1 = TF.center_crop(dec1, x.shape[2:])  # Use center crop to match the original input size\n",
        "        # dec1 = torch.cat((x, dec1), dim=1)  # Skip connection with the original image\n",
        "        # print(f\"After skip: {dec1.shape}\")\n",
        "        dec1 = self.decoder1(dec2)\n",
        "        # print(f\"After dec1: {dec1.shape}\")# (1, 64, 480, 640)\n",
        "\n",
        "        out = self.final_conv(dec1)  # (1, output_ch, 480, 640)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEUDKukVFS6P"
      },
      "source": [
        "### D. Define Experiment/Training details here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1geuNqp-I-K"
      },
      "outputs": [],
      "source": [
        "# Define Experiment Configurations (from config.py)\n",
        "class Experiment:\n",
        "    def __init__(self, config_name):\n",
        "        \"\"\"\n",
        "        Initializes the experiment based on the configuration name.\n",
        "\n",
        "        Parameters:\n",
        "        - config_name: The name of the experiment configuration to use.\n",
        "\n",
        "        This constructor sets up the model, training parameters, datasets, and loss function\n",
        "        based on the specific configuration.\n",
        "        \"\"\"\n",
        "        if config_name == \"MetricWeightedLossBlenderNYU\":\n",
        "            # Set the model architecture (e.g., U-Net) for this experiment\n",
        "            self.model = U_Net\n",
        "            # Defome Number of training epochs\n",
        "            self.epochs = 45\n",
        "            # Define Batch size for training\n",
        "            self.batch_size = 1\n",
        "            # Define Learning rate for the optimizer\n",
        "            self.learning_rate = 0.0001\n",
        "            # Modify the list of training datasets with specific configurations\n",
        "            self.train_datasets = [\n",
        "                {\"nyu_data\": [\"rgb\", 1000, False]},  # Dataset name: Coded Images Subfolder, scale factor, is_blender flag\n",
        "                {\"LivingRoom1\": [\"rgb\", 1, True]}  #  Dataset name: Coded Images Subfolder, scale factor, is_blender flag\n",
        "            ]\n",
        "\n",
        "            # Modify the list of test datasets with specific configurations\n",
        "            self.test_datasets = [\n",
        "                # Test dataset configuration examples:\n",
        "                # {\"Corridor\": [\"Codedphasecam-27Linear\", 1, True]},\n",
        "                {\"DiningRoom\": [\"Codedphasecam-27Linear-New\", 1, True]}  # Dataset for testing\n",
        "                # Additional test datasets can be added here\n",
        "            ]\n",
        "            # Set the loss function to the custom weighted mean squared error (MSE) loss\n",
        "            self.loss_fn = self.my_loss\n",
        "        # Add more configurations as needed for different experiments\n",
        "\n",
        "    def my_loss(self, output, target):\n",
        "        \"\"\"\n",
        "        Write a loss function.\n",
        "\n",
        "        Parameters:\n",
        "        - output: The predicted output from the model.\n",
        "        - target: The ground truth depth values.\n",
        "\n",
        "        Returns:\n",
        "        The loss value.\n",
        "        \"\"\"\n",
        "        weighted_depth = 2**(0.3*target)\n",
        "        weighted_mse = weighted_depth * (output - target) ** 2\n",
        "\n",
        "    # Reduce the loss (either using mean or sum)\n",
        "        loss = torch.mean(weighted_mse)\n",
        "        # Calculate the loss based on target values\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9EjTDd1_LS6"
      },
      "outputs": [],
      "source": [
        "# Helper Functions\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts the number of trainable parameters in a PyTorch model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The PyTorch model whose parameters are to be counted.\n",
        "\n",
        "    Returns:\n",
        "    The total number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def init_weights(net, init_type=\"normal\", gain=0.02):\n",
        "    \"\"\"\n",
        "    Initializes the weights of the network layers based on the specified initialization type.\n",
        "\n",
        "    Parameters:\n",
        "    - net: The neural network (PyTorch model) whose weights are to be initialized.\n",
        "    - init_type: The type of weight initialization ('normal', 'xavier', 'kaiming', or 'orthogonal').\n",
        "    - gain: A scaling factor for the initialization (applies to certain initialization methods).\n",
        "\n",
        "    This function defines an internal function `init_func` that is applied to each layer of the network.\n",
        "    The weights of convolutional and linear layers are initialized based on the `init_type`,\n",
        "    while batch normalization layers have their weights and biases initialized separately.\n",
        "    \"\"\"\n",
        "\n",
        "    def init_func(m):\n",
        "        \"\"\"\n",
        "        Applies the initialization function to each layer `m` in the network.\n",
        "\n",
        "        This function checks the type of layer (Conv, Linear, BatchNorm2d) and applies the\n",
        "        appropriate initialization method to its weights and biases.\n",
        "        \"\"\"\n",
        "        classname = m.__class__.__name__  # Get the class name of the layer\n",
        "        if hasattr(m, \"weight\") and (classname.find(\"Conv\") != -1 or classname.find(\"Linear\") != -1):\n",
        "            # If the layer has a 'weight' attribute and is a Conv or Linear layer\n",
        "            if init_type == \"normal\":\n",
        "                nn.init.normal_(m.weight.data, 0.0, gain)  # Normal distribution initialization\n",
        "            elif init_type == \"xavier\":\n",
        "                nn.init.xavier_normal_(m.weight.data, gain=gain)  # Xavier initialization\n",
        "            elif init_type == \"kaiming\":\n",
        "                nn.init.kaiming_normal_(m.weight.data, a=0, mode=\"fan_in\")  # Kaiming (He) initialization\n",
        "            elif init_type == \"orthogonal\":\n",
        "                nn.init.orthogonal_(m.weight.data, gain=gain)  # Orthogonal initialization\n",
        "\n",
        "            # If the layer has a bias term, initialize it to 0\n",
        "            if hasattr(m, \"bias\") and m.bias is not None:\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "        # If the layer is a BatchNorm2d layer, initialize its weight to 1 and bias to 0\n",
        "        elif classname.find(\"BatchNorm2d\") != -1:\n",
        "            nn.init.normal_(m.weight.data, 1.0, gain)  # Initialize BatchNorm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRWbZTGRH2D7"
      },
      "source": [
        "## E: Training Loop Goes here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGtX8TDM_iXp"
      },
      "outputs": [],
      "source": [
        "# Training Loop Function\n",
        "def train(model, dataloader, test_loader, optimizer, criterion, epochs, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Trains the model over multiple epochs and saves the best-performing model based on the loss.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The neural network model to be trained.\n",
        "    - dataloader: DataLoader for the training data.\n",
        "    - test_loader: DataLoader for the test/validation data (if evaluation is done during training).\n",
        "    - optimizer: Optimizer for updating the model's weights.\n",
        "    - criterion: Loss function used for training (e.g., MSELoss).\n",
        "    - epochs: Number of epochs to train the model.\n",
        "    - checkpoint_path: Path to save model checkpoints during training.\n",
        "    \"\"\"\n",
        "\n",
        "    epoch_start = 0  # Starting epoch (useful for resuming training)\n",
        "    best_loss = float('inf')  # Initialize best loss with a very large value for comparison\n",
        "\n",
        "    # Load checkpoint if it exists (for resuming training from the last saved state)\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])  # Load model weights\n",
        "        epoch_start = checkpoint['epoch']  # Load the last epoch completed\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # Load optimizer state\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(epoch_start, epochs):\n",
        "        print(\"epoch: \", epoch)\n",
        "        model.train()  # Set model to training mode\n",
        "        total_loss = 0  # Reset total loss for the current epoch\n",
        "\n",
        "        # Loop over each batch in the dataloader\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad()  # Reset the gradients from the previous step\n",
        "            inputs, targets = batch['Coded'].to(device), batch['Depth'].to(device)  # Move input and target to the device (GPU/CPU)\n",
        "            # print(\"Input\",inputs.shape)\n",
        "            # print(\"Target\",targets.shape)\n",
        "            outputs = model(inputs)  # Forward pass: compute model predictions\n",
        "            targets = targets.unsqueeze(1)  # Add channel dimension to targets (for compatibility with model output)\n",
        "            loss = criterion(outputs, targets)  # Compute the loss between outputs and targets\n",
        "            loss.backward()  # Backpropagation: compute gradients\n",
        "            optimizer.step()  # Update model weights based on computed gradients\n",
        "            total_loss += loss.item()  # Accumulate the loss for the current batch\n",
        "\n",
        "        # Calculate the average loss for the epoch\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss}\")\n",
        "\n",
        "        # Save model checkpoint after each epoch\n",
        "        torch.save({\n",
        "            'epoch': epoch,  # Save the current epoch\n",
        "            'model_state_dict': model.state_dict(),  # Save model's state (weights)x``\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Save optimizer's state\n",
        "            'loss': avg_loss,  # Save the average loss for this epoch\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        # Save the best model checkpoint if the current epoch has the lowest loss so far\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss  # Update best loss\n",
        "            best_checkpoint_path = checkpoint_path.replace('.pt', '_best.pt')  # Create a file name for the best checkpoint\n",
        "            torch.save({\n",
        "                'epoch': epoch,  # Save the current epoch\n",
        "                'model_state_dict': model.state_dict(),  # Save the model's state (weights)\n",
        "                'optimizer_state_dict': optimizer.state_dict(),  # Save the optimizer's state\n",
        "                'loss': avg_loss,  # Save the average loss for this epoch\n",
        "            }, best_checkpoint_path)  # Save the best model as a separate file\n",
        "            print(f\"New best model saved with loss {best_loss:.4f}\")\n",
        "\n",
        "        # Uncomment this block if you want to evaluate the model after every few epochs\n",
        "        # This block will run the validation step every 5 epochs and print the test loss\n",
        "        # if epoch % 5 == 0:\n",
        "        #   test_l1, test_l1_under3 = validate(model, test_loader, nn.L1Loss(), device)\n",
        "        #   print(f\"Test L1 error: {test_l1}\")\n",
        "        #   print(f\"Test L1 error for depth < 3m: {test_l1_under3}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LssqlC48Ijty"
      },
      "source": [
        "## F: Evaluation Metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOgVAu6POdUj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# Define metrics\n",
        "def abs_rel(pred, target):\n",
        "    return torch.mean(torch.abs(target - pred) / target)\n",
        "\n",
        "def rmse(pred, target):\n",
        "    return torch.sqrt(torch.mean((target - pred) ** 2))\n",
        "\n",
        "# Convert tensor to numpy\n",
        "def to_numpy(img: torch.Tensor):\n",
        "    return np.clip(img.detach().cpu().numpy(), 0, 6)  # Clip to range [0m, 6m]\n",
        "\n",
        "# Evaluate Function\n",
        "def evaluate(model, dataloader, device, output_dir, color_map='viridis'):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    abs_rel_errors = []\n",
        "    threshold_accs = []\n",
        "    rms_errors = []\n",
        "    inference_times = []\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        # Get the input image and ground truth depth from the batch\n",
        "        input_image = batch['Coded'].to(device)\n",
        "        true_depth = batch['Depth'].to(device)\n",
        "\n",
        "        # Start inference timer\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Forward pass through the model to get predicted depth\n",
        "        with torch.no_grad():\n",
        "            predicted_depth = model(input_image)\n",
        "\n",
        "        # Stop inference timer and calculate inference time\n",
        "        end_time = time.time()\n",
        "        inference_times.append(end_time - start_time)\n",
        "\n",
        "        # Compute metrics\n",
        "        abs_rel_errors.append(abs_rel(predicted_depth, true_depth).item())\n",
        "        rms_errors.append(rmse(predicted_depth, true_depth).item())\n",
        "\n",
        "        # Convert the predicted depth to numpy, clip, and prepare for color mapping\n",
        "        pred_depth_np = to_numpy(predicted_depth[0, 0])  # Convert the first image in batch\n",
        "\n",
        "        # Plot with color map and add color bar\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        img = plt.imshow(pred_depth_np, cmap=cm.get_cmap(color_map), vmin=0, vmax=6)\n",
        "        cbar = plt.colorbar(img, fraction=0.046, pad=0.04)\n",
        "        cbar.set_label('Depth (meters)')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Save image with color bar as PNG\n",
        "        plt.savefig(os.path.join(output_dir, f'pred_depth_{i}.png'), bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_abs_rel = np.mean(abs_rel_errors)\n",
        "    avg_rmse = np.mean(rms_errors)\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "    fps = 1.0 / avg_inference_time if avg_inference_time > 0 else float('inf')\n",
        "\n",
        "    average_metrics = {\n",
        "        'Abs-Rel': avg_abs_rel,\n",
        "        'RMSE': avg_rmse,\n",
        "        'FPS': fps\n",
        "    }\n",
        "\n",
        "    return average_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gssTC6foIhlK"
      },
      "source": [
        "## G: Putting everything together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_599OFJANhC",
        "outputId": "28013763-4afa-4df8-9d28-133cd948052c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rgb False 1000\n",
            "1000\n",
            "rgb True 1\n",
            "1000\n",
            "Codedphasecam-27Linear-New True 1\n",
            "999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:18: SyntaxWarning: invalid escape sequence '\\A'\n",
            "<>:18: SyntaxWarning: invalid escape sequence '\\A'\n",
            "C:\\Users\\sktha\\AppData\\Local\\Temp\\ipykernel_31912\\1093368701.py:18: SyntaxWarning: invalid escape sequence '\\A'\n",
            "  checkpoint_path = f'C:\\Adv Computer Vision\\P2\\model_checkpoints\\model_sect1_{experiment.epochs}'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In train loader - Batches: 2000 Samples:  2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sktha\\AppData\\Local\\Temp\\ipykernel_31912\\1093368701.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(checkpoint_path, map_location=device)['model_state_dict'])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating for dataset: DiningRoom\n",
            "Codedphasecam-27Linear-New True 1\n",
            "999\n",
            "outputdir <torch.utils.data.dataloader.DataLoader object at 0x000002453535D010>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sktha\\AppData\\Local\\Temp\\ipykernel_31912\\1394826309.py:58: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "  img = plt.imshow(pred_depth_np, cmap=cm.get_cmap(color_map), vmin=0, vmax=6)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Abs-Rel': np.float64(0.45316312734309855), 'RMSE': np.float64(1.1641531132004999), 'FPS': np.float64(348.4987330448161)}\n"
          ]
        }
      ],
      "source": [
        "# Main Execution\n",
        "def main(config_name, dataset_path, test_dataset_path):\n",
        "    \"\"\"\n",
        "    Main function to run the experiment.\n",
        "\n",
        "    Parameters:\n",
        "    - config_name: Name of the experiment configuration to use.\n",
        "    - dataset_path: Path to the training dataset.\n",
        "    - test_dataset_path: Path to the test dataset.\n",
        "\n",
        "    This function loads the datasets, initializes the model, and evaluates it on the test data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the experiment configuration (model, datasets, loss function, etc.)\n",
        "    experiment = Experiment(config_name)\n",
        "\n",
        "    # Define the checkpoint path where the model is saved\n",
        "    checkpoint_path = f'C:\\Adv Computer Vision\\P2\\model_checkpoints\\model_sect1_{experiment.epochs}'\n",
        "\n",
        "    # Load training datasets using the configurations defined in the experiment\n",
        "    # Each dataset is created using ImageDepthDataset and stored in train_datasets list\n",
        "    train_datasets = [\n",
        "        ImageDepthDataset(\n",
        "            os.path.join(dataset_path, list(data_d.keys())[0]),  # Path to the dataset\n",
        "            codedDir=list(data_d.values())[0][0],  # Directory containing coded images\n",
        "            cache=False,  # Whether to cache the dataset in memory\n",
        "            scale_factor=list(data_d.values())[0][1],  # Scale factor for depth images\n",
        "            is_blender=list(data_d.values())[0][2],  # Whether the data is from Blender (EXR format)\n",
        "            limit=1000  # Limit the number of images to load (optional)\n",
        "        )\n",
        "        for data_d in experiment.train_datasets  # Loop through the training datasets in the experiment config\n",
        "    ]\n",
        "\n",
        "    # Create a DataLoader to iterate over the combined training datasets (shuffled)\n",
        "    train_loader = DataLoader(ConcatDataset(train_datasets), batch_size=experiment.batch_size, shuffle=True)\n",
        "\n",
        "    # Load test datasets using the configurations defined in the experiment\n",
        "    test_datasets = [\n",
        "        ImageDepthDataset(\n",
        "            os.path.join(test_dataset_path, list(data_d.keys())[0]),  # Path to the dataset\n",
        "            codedDir=list(data_d.values())[0][0],  # Directory containing coded images\n",
        "            cache=False,  # Whether to cache the dataset in memory\n",
        "            scale_factor=list(data_d.values())[0][1],  # Scale factor for depth images\n",
        "            is_blender=list(data_d.values())[0][2]  # Whether the data is from Blender (EXR format)\n",
        "        )\n",
        "        for data_d in experiment.test_datasets  # Loop through the test datasets in the experiment config\n",
        "    ]\n",
        "\n",
        "    # Create a DataLoader to iterate over the combined test datasets (not shuffled)\n",
        "    test_loader = DataLoader(ConcatDataset(test_datasets), batch_size=1, shuffle=False)\n",
        "\n",
        "    # Initialize the model from the experiment configuration and move it to the specified device (CPU/GPU)\n",
        "    model = experiment.model().to(device)\n",
        "\n",
        "    # Initialize model weights\n",
        "    init_weights(model)\n",
        "    parameters =   model.parameters()\n",
        "    # Define the optimizer (Adam in this case) with the learning rate from the experiment config\n",
        "    # Optimizer goes here. Adam optimizer is recommended.\n",
        "    optimizer = torch.optim.Adam(parameters,lr=0.0001)\n",
        "\n",
        "    print(\"In train loader - Batches:\", len(train_loader), \"Samples: \", len(train_loader) * experiment.batch_size)\n",
        "\n",
        "    # Uncomment the line below to start training the model (currently commented for evaluation only)\n",
        "    # train(model, train_loader, test_loader, optimizer, experiment.loss_fn, experiment.epochs, checkpoint_path)\n",
        "\n",
        "    # Load the pre-trained model from the checkpoint\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device)['model_state_dict'])\n",
        "\n",
        "    # Loop through each test dataset for evaluation\n",
        "    for data_d in experiment.test_datasets:\n",
        "        print(f\"Evaluating for dataset: {list(data_d.keys())[0]}\")\n",
        "\n",
        "        # Load the evaluation dataset using the same configuration as the test datasets\n",
        "        eval_datasets = ImageDepthDataset(\n",
        "            os.path.join(test_dataset_path, list(data_d.keys())[0]),  # Path to the dataset\n",
        "            codedDir=list(data_d.values())[0][0],  # Directory containing coded images\n",
        "            cache=False,  # Whether to cache the dataset in memory\n",
        "            scale_factor=list(data_d.values())[0][1],  # Scale factor for depth images\n",
        "            is_blender=list(data_d.values())[0][2]  # Whether the data is from Blender (EXR format)\n",
        "        )\n",
        "\n",
        "        # Create a DataLoader for the evaluation dataset (batch size 16, no shuffle)\n",
        "        eval_loader = DataLoader(eval_datasets, batch_size=16, shuffle=False)\n",
        "\n",
        "        # Define the output directory for saving predicted depth images\n",
        "        output_dir = f'./CodedVO_pred_Sect1_{experiment.epochs}_new_colormap' + list(data_d.keys())[0]\n",
        "        os.makedirs(os.path.join(output_dir, \"pred_depth\"), exist_ok=True)  # Create output directory if it doesn't exist\n",
        "        print(\"outputdir\",test_loader)\n",
        "        # Evaluate the model on the test data and compute metrics\n",
        "        metrics = evaluate(model, test_loader, device, output_dir)\n",
        "\n",
        "        # Print evaluation metrics\n",
        "        print(metrics)\n",
        "\n",
        "# If running from command-line, the main function is executed with the given configuration\n",
        "if __name__ == \"__main__\":\n",
        "    main('MetricWeightedLossBlenderNYU', './train/', './UMD-CodedVO-dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqranWcFI2GI"
      },
      "source": [
        "# Submission Guidelines:\n",
        "\n",
        "1. Compiled version of the colab file with results (Feel free to use two colab files if you wish, one for AiF training/testing and other one for coded and comparison)\n",
        "2. PDF Report (Combined with Part 1)\n",
        "3. Output predicted depth maps on testing dataset (ICL and DiningRoom) of at least 5 images each must be present in your report.\n",
        "4. Compare extensively on both AiF and Coded Images Training/Test both qualitatively and quantitatively. Make sure they are in the metric units and not normalized.\n",
        "\n",
        "Note: You should see reasonable results after 15 epochs but almost as good results as in the CodedVO paper by 40 epochs.\n",
        "\n",
        "***\n",
        "The following are the out of domain testing on an image from UMDCodedVO-DiningRoom dataset at different epochs:\n",
        "\n",
        "GT Depth Image:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=15i2EPFdAWbWECAoE1UVSYryqlZZOm_Md\" width=\"300\">\n",
        "\n",
        "Depth Prediction after 5 epochs:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1qIwRiMQ-PVY8MgrlaA1-jtkeUjj9IQej\" width=\"300\">\n",
        "\n",
        "Depth Prediction after 15 epochs:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1udrXdNa10RJG8kA8fj7M1Y3auv_ggJuz\" width=\"300\">\n",
        "\n",
        "Depth Prediction after 30 epochs:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1LdGneqtMW7RCxthBtQh_Q28OzxYkpNfc\" width=\"300\">\n",
        "\n",
        "Depth Prediction after 45 epochs:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=155tUgHJT2mX0j1EvfOaJ8mP34hpJbpV3\" width=\"300\">\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}